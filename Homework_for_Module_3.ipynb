{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT/TLCBT5jMBxfj6UJ261R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MCn21thCntry/Practical-Machine-Learning---from-the-rooter-to-the-tooter/blob/main/Homework_for_Module_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Conceptual Understanding (Answer in Markdown Text)\n",
        "\n",
        "\n",
        "1.   Decision Tree Flowchart - Explain the Learning Process: Explain in your own words how a Decision Tree Regressor learns to make predictions. Use the \"flowchart\" analogy to describe the learning process and how the model uses features to make decisions and reach a prediction.\n",
        "  *   It starts with all features\n",
        "  *   Then, it finds the best feature and splits it two branches\n",
        "  *   It continues to splitting the each node and generates two branches for each node\n",
        "  *   It continues until there is nothing to leave from the features according to splitting in two or we can decide min_sample_split parameter to specify where to stop.\n",
        "  *   Then, model uses features to make decisions at each split to avoid having big loss that means difference between prediction and real value.\n",
        "\n",
        "2.   Overfitting - Explain in Practical Terms and Relate to Module 3: Explain the concept of \"overfitting\" and how it was hinted at in Module 3, especially with the \"Original Model\" achieving near-perfect predictions on the training data. Why is overfitting a problem for real-world Machine Learning applications?\n",
        "  *   Overfitting means that model performes good on training data but bad on test or unseen data.\n",
        "  *   Overfitting is a problem because the model predicts mostly wrong predictions than what it could be so it means that model couldnot generalize to new data. In other words, it just fits itself on tight sample not on population.\n",
        "\n",
        "https://www.kaggle.com/code/khaledhijja/module-3-machine-learning-fundamentals-decision-tr\n",
        "3.   Importance of Feature Relevance - Exercise 4 Takeaway: Based on your experience with Exercise 4 (Predicting with Unseen Features), explain in detail why feature relevance is so crucial for Machine Learning model performance. What did Exercise 4 demonstrate about the relationship between the features a model is trained on and its ability to make predictions?\n",
        "  *   Feature relevance is so crucial because unseen features cause the model to generate very far away predictions from the real values. It kinda causes hallucination which even unwelcomed in human life.\n",
        "  *   Exercise 4 shows that the new features unseen by our model can not be predicted and generalized. So, ML models are trained on features' relationships which means that every model should have the feature before and after trainig in order to predict/guess on it.\n",
        "\n",
        "\n",
        "4.   Train-Test Split - Why is it Essential for Evaluation? Explain in your own words why using a train-test split (as introduced in Exercise 5) is a more reliable way to evaluate the performance of a Machine Learning model than simply evaluating it on the training data. What specific aspect of model performance does train-test split help us to assess?\n",
        "  *   Train test split is essential for evaluation because it provides to generate more reliable predictions on test data when comparing the model based on all data.\n",
        "  *   By splitting the data, the model's performance is assessed on data that hasn't seen before. It means that the predictions will be similar to the real values.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozZuaVQ2Si3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Code Exercises - Focused Exploration (Modify the Code and Explain in Markdown Text)"
      ],
      "metadata": {
        "id": "8O-QlIB9AKeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run and Analyze Exercise 3 - \"Optimal\" Feature Set:\n",
        "\n",
        "Task: Run Code Cells 17-20 from the Module 3 Colab Notebook, which correspond to Exercise 3 - \"Optimal\" Feature Set (Features: ['OverallQual', 'GrLivArea', 'HouseAge']).\n",
        "\n",
        "Explanation: In a Markdown cell after Code Cell 20, summarize your observations on the predictions from this \"optimal\" model. Do these predictions appear subjectively \"good\" for the first 5 houses? Why might this feature set be considered \"optimal\" (based on our limited evaluation)?"
      ],
      "metadata": {
        "id": "khoncdNkA8tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Reach the data in your drive\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z36ivldBIMt",
        "outputId": "6724b72c-8c2f-413b-98e8-7806926e5428"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from a CSV file in your Drive\n",
        "iowa_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Iowa Housing Dataset/train.csv')\n",
        "iowa_data.head()\n",
        "iowa_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAJygEbbD78p",
        "outputId": "77e3f4a7-e52b-4728-f093-32626425ba11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Order', 'PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area',\n",
              "       'Street', 'Alley', 'Lot Shape', 'Land Contour', 'Utilities',\n",
              "       'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',\n",
              "       'Condition 2', 'Bldg Type', 'House Style', 'Overall Qual',\n",
              "       'Overall Cond', 'Year Built', 'Year Remod/Add', 'Roof Style',\n",
              "       'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',\n",
              "       'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation', 'Bsmt Qual',\n",
              "       'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1',\n",
              "       'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF',\n",
              "       'Heating', 'Heating QC', 'Central Air', 'Electrical', '1st Flr SF',\n",
              "       '2nd Flr SF', 'Low Qual Fin SF', 'Gr Liv Area', 'Bsmt Full Bath',\n",
              "       'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr',\n",
              "       'Kitchen AbvGr', 'Kitchen Qual', 'TotRms AbvGrd', 'Functional',\n",
              "       'Fireplaces', 'Fireplace Qu', 'Garage Type', 'Garage Yr Blt',\n",
              "       'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual',\n",
              "       'Garage Cond', 'Paved Drive', 'Wood Deck SF', 'Open Porch SF',\n",
              "       'Enclosed Porch', '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Pool QC',\n",
              "       'Fence', 'Misc Feature', 'Misc Val', 'Mo Sold', 'Yr Sold', 'Sale Type',\n",
              "       'Sale Condition', 'SalePrice'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##1 To understand the concept of generating model-Not seen in real world scenarios\n",
        "selected_features = ['Lot Area', 'Overall Qual', 'Gr Liv Area', 'Garage Cars', 'Total Bsmt SF', 'Year Built']\n",
        "X_predictions = iowa_data[selected_features]\n",
        "y = iowa_data['SalePrice']\n",
        "iowa_model_predictions = DecisionTreeRegressor()\n",
        "iowa_model_predictions.fit(X_predictions, y)\n",
        "predictions = iowa_model_predictions.predict(X_predictions)\n",
        "\n",
        "##2 Model Generation with Feature Extraction\n",
        "# 1. Create 'QualityXArea' feature\n",
        "quality_x_area = iowa_data['Overall Qual'] * iowa_data['Gr Liv Area']\n",
        "# 2. Create X_exercise_2 DataFrame with 'QualityXArea' and other selected features\n",
        "selected_features_exercise_2 = ['Lot Area', 'Garage Cars', 'Total Bsmt SF', 'Year Built'] # Example: Some original features\n",
        "X_exercise_2 = iowa_data[selected_features_exercise_2].copy() # Start with selected original features\n",
        "X_exercise_2['QualityXArea'] = quality_x_area # Add 'QualityXArea' column\n",
        "iowa_model_exercise_2 = DecisionTreeRegressor()\n",
        "iowa_model_exercise_2.fit(X_exercise_2, y)\n",
        "predictions_exercise_2 = iowa_model_exercise_2.predict(X_exercise_2) # Using X_exercise_2 from Exercise 2"
      ],
      "metadata": {
        "id": "8g8A8ep-FusU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Run and Analyze Exercise 3 - \"Optimal\" Feature Set:\n",
        "Task: Run Code Cells 17-20 from the Module 3 Colab Notebook, which correspond to Exercise 3 - \"Optimal\" Feature Set (Features: ['OverallQual', 'GrLivArea', 'HouseAge']).\n",
        "\"\"\"\n",
        "# Create 'HouseAge' column first\n",
        "if 'Year Built' in iowa_data.columns: # Ensure YearBuilt column exists\n",
        "    iowa_data['House Age'] = 2010 - iowa_data['Year Built']\n",
        "else:\n",
        "    raise KeyError(\"YearBuilt column not found in iowa_data\")\n",
        "\n",
        "selected_features_exercise_3 = ['Overall Qual', 'Gr Liv Area', 'House Age'] # Subjectively 'optimal' feature set\n",
        "X_exercise_3 = iowa_data[selected_features_exercise_3].copy() # Using HouseAge which needs to be created first (if not already)\n",
        "iowa_model_exercise_3 = DecisionTreeRegressor()\n",
        "iowa_model_exercise_3.fit(X_exercise_3, y)\n",
        "predictions_exercise_3 = iowa_model_exercise_3.predict(X_exercise_3)\n",
        "\n",
        "print(\"Predictions (Exercise 3) created successfully!\")\n",
        "print(\"\\nFirst 5 predictions (Exercise 3):\")\n",
        "print(predictions_exercise_3[:5])\n",
        "comparison_df_exercise_3 = pd.DataFrame({'Actual': y[:50], 'Predicted': predictions[:50], 'Predicted (Exercise 2)': predictions_exercise_2[:50], 'Predicted (Exercise 3)': predictions_exercise_3[:50]})\n",
        "print(comparison_df_exercise_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia31IW5UDK4U",
        "outputId": "36c85646-b513-4783-f439-1b99a91cd788"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions (Exercise 3) created successfully!\n",
            "\n",
            "First 5 predictions (Exercise 3):\n",
            "[159000. 271900. 137500. 248500. 167000.]\n",
            "    Actual  Predicted  Predicted (Exercise 2)  Predicted (Exercise 3)\n",
            "0   159000   159000.0                159000.0                159000.0\n",
            "1   271900   271900.0                271900.0                271900.0\n",
            "2   137500   137500.0                137500.0                137500.0\n",
            "3   248500   248500.0                248500.0                248500.0\n",
            "4   167000   167000.0                167000.0                167000.0\n",
            "5   140000   140000.0                140000.0                140000.0\n",
            "6   128000   128000.0                128000.0                128000.0\n",
            "7   213000   213000.0                213000.0                213000.0\n",
            "8   100000   100000.0                100000.0                100000.0\n",
            "9   190000   190000.0                190000.0                190000.0\n",
            "10  225000   225000.0                225000.0                225000.0\n",
            "11   94900    94900.0                 94900.0                 99950.0\n",
            "12  231500   231500.0                231500.0                231500.0\n",
            "13  180000   180000.0                180000.0                180000.0\n",
            "14  410000   410000.0                410000.0                410000.0\n",
            "15  184500   184500.0                184500.0                184500.0\n",
            "16  127000   127000.0                127000.0                127000.0\n",
            "17  120000   120000.0                120000.0                120000.0\n",
            "18  154000   154000.0                154000.0                154000.0\n",
            "19  137500   137500.0                137500.0                137500.0\n",
            "20   87000    87000.0                 87000.0                 87000.0\n",
            "21  161000   161000.0                161000.0                161000.0\n",
            "22   68500    68500.0                 68500.0                 68500.0\n",
            "23  301600   301600.0                301600.0                301600.0\n",
            "24  169000   169000.0                169000.0                169000.0\n",
            "25  387000   387000.0                387000.0                387000.0\n",
            "26  127500   127500.0                127500.0                127500.0\n",
            "27  230000   230000.0                230000.0                230000.0\n",
            "28  332200   332200.0                332200.0                332200.0\n",
            "29   90000    90000.0                 90000.0                 90000.0\n",
            "30  755000   755000.0                755000.0                755000.0\n",
            "31  155835   155835.0                155835.0                155835.0\n",
            "32  279000   279000.0                279000.0                279000.0\n",
            "33  128000   128000.0                128000.0                128000.0\n",
            "34  193500   193500.0                193500.0                193500.0\n",
            "35  140000   140000.0                140000.0                140000.0\n",
            "36  187000   187000.0                187000.0                187000.0\n",
            "37  195500   195500.0                195500.0                195500.0\n",
            "38  119000   119000.0                119000.0                121500.0\n",
            "39   76000    76000.0                 76000.0                 80010.0\n",
            "40   82500    82500.0                 82500.0                 82500.0\n",
            "41  190000   190000.0                190000.0                190000.0\n",
            "42  230000   230000.0                230000.0                230000.0\n",
            "43   91900    91900.0                 91900.0                 91900.0\n",
            "44  173000   173000.0                173000.0                173000.0\n",
            "45  257000   257000.0                257000.0                257000.0\n",
            "46  127000   127000.0                127000.0                127000.0\n",
            "47  269500   269500.0                269500.0                269500.0\n",
            "48  260000   260000.0                260000.0                260000.0\n",
            "49  158450   158450.0                158450.0                158450.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: In a Markdown cell after Code Cell 20, summarize your observations on the predictions from this \"optimal\" model. Do these predictions appear subjectively \"good\" for the first 5 houses? Why might this feature set be considered \"optimal\" (based on our limited evaluation)?\n",
        "\n",
        "    * We got totally the same results with real values so it appears subjectively good but it is objectively bad."
      ],
      "metadata": {
        "id": "56CUZEx-DZgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run and Analyze Exercise 4 - \"Unseen Features\" Experiment:"
      ],
      "metadata": {
        "id": "iUacGqDeA8fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Task: Run Code Cells 21-23 from the Module 3 Colab Notebook, which correspond to Exercise 4 (Predicting with Unseen Features). \"\"\"\n",
        "selected_features_unseen = ['Overall Qual', 'Gr Liv Area', 'Lot Frontage', 'Mas Vnr Area'] # Mismatched feature set\n",
        "X_unseen_features = iowa_data[selected_features_unseen].copy()\n",
        "# --- Imputation ---\n",
        "# Impute missing values using the mean\n",
        "X_unseen_features = X_unseen_features.fillna(X_unseen_features.mean())\n",
        "iowa_model_exercise_4 = DecisionTreeRegressor()\n",
        "iowa_model_exercise_4.fit(X_unseen_features, y)\n",
        "\n",
        "selected_features_to_test_predictions = ['Overall Qual', 'Gr Liv Area', 'Lot Frontage', 'Mas Vnr Area'] # Mismatched feature set\n",
        "X_unseen_features_to_test_predictions = iowa_data[selected_features_to_test_predictions].copy()\n",
        "# --- Imputation ---\n",
        "# Impute missing values using the mean\n",
        "X_unseen_features_to_test_predictions = X_unseen_features_to_test_predictions.fillna(X_unseen_features_to_test_predictions.mean())\n",
        "predictions_unseen_features = iowa_model_exercise_4.predict(X_unseen_features_to_test_predictions)\n",
        "\n",
        "comparison_df_unseen_features = pd.DataFrame({'Actual': y[:10], 'Predicted (Mismatched Features)': predictions_unseen_features[:10], 'Predicted': predictions[:10], 'Predicted (Exercise 2)': predictions_exercise_2[:10], 'Predicted (Exercise 3)': predictions_exercise_3[:10]})\n",
        "print(comparison_df_unseen_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi7RVG6rPrxh",
        "outputId": "7c84b0fe-34c0-4248-a1a3-66ffb33ca617"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Actual  Predicted (Mismatched Features)  Predicted  Predicted (Exercise 2)  \\\n",
            "0  159000                         159000.0   159000.0                159000.0   \n",
            "1  271900                         271900.0   271900.0                271900.0   \n",
            "2  137500                         137500.0   137500.0                137500.0   \n",
            "3  248500                         248500.0   248500.0                248500.0   \n",
            "4  167000                         167000.0   167000.0                167000.0   \n",
            "5  140000                         140000.0   140000.0                140000.0   \n",
            "6  128000                         128000.0   128000.0                128000.0   \n",
            "7  213000                         213000.0   213000.0                213000.0   \n",
            "8  100000                         100000.0   100000.0                100000.0   \n",
            "9  190000                         190000.0   190000.0                190000.0   \n",
            "\n",
            "   Predicted (Exercise 3)  \n",
            "0                159000.0  \n",
            "1                271900.0  \n",
            "2                137500.0  \n",
            "3                248500.0  \n",
            "4                167000.0  \n",
            "5                140000.0  \n",
            "6                128000.0  \n",
            "7                213000.0  \n",
            "8                100000.0  \n",
            "9                190000.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: In a Markdown cell after Code Cell 23, explain your observations from Exercise 4. How do the predictions made with the \"mismatched\" feature set compare to the actual values? Why is the model likely performing poorly in this scenario? What key lesson does this exercise illustrate about Machine Learning models and their dependence on training features?\n",
        "\n",
        "    * It looks like both gives the same value eventhough they were trained on mismatched features\n",
        "    but it shouldn't be forgotten that they also were evaluated on the same dataset."
      ],
      "metadata": {
        "id": "26WXsJe0GR5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run and Analyze Exercise 5 - Train-Test Split Evaluation:"
      ],
      "metadata": {
        "id": "Pz-Lmeq9BObV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Task: Run Code Cells 24-27 from the Module 3 Colab Notebook, which correspond to Exercise 5 (Train-Test Split Evaluation).\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We will use the \"optimal\" features (X_exercise_3) and target (y) for this exercise.\n",
        "# Recall: X_exercise_3 = iowa_data[['OverallQual', 'GrLivArea', 'HouseAge']]\n",
        "#         y = iowa_data['SalePrice']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_exercise_3, y, test_size=0.9, random_state=0)\n",
        "# - X_train, y_train:  Features and target for the training set.\n",
        "# - X_test, y_test: Features and target for the testing set (unseen data).\n",
        "# - test_size=0.2:  20% of the data will be used for testing, 80% for training.\n",
        "# - random_state=0:  Ensures the split is the same every time you run the code (for reproducibility).\n",
        "\n",
        "print(\"Data split into training and testing sets successfully!\")\n",
        "print(\"\\nShape of X_train:\", X_train.shape) # Shape of training features\n",
        "print(\"Shape of X_test:\", X_test.shape)   # Shape of testing features\n",
        "print(\"Shape of y_train:\", y_train.shape) # Shape of training target\n",
        "print(\"Shape of y_test:\", y_test.shape)   # Shape of testing target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHOKNOFY9f7R",
        "outputId": "1cf9ee9a-9d5c-4477-be24-099ed9d59e71"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and testing sets successfully!\n",
            "\n",
            "Shape of X_train: (219, 3)\n",
            "Shape of X_test: (1978, 3)\n",
            "Shape of y_train: (219,)\n",
            "Shape of y_test: (1978,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iowa_model_test_exercise = DecisionTreeRegressor()\n",
        "iowa_model_test_exercise.fit(X_train, y_train) # IMPORTANT: Fit ONLY on training data\n",
        "\n",
        "predictions_test_exercise = iowa_model_test_exercise.predict(X_test) # IMPORTANT: Predict on TESTING features (X_test)\n",
        "\n",
        "print(\"Predictions on TESTING data (unseen data) created successfully!\")\n",
        "print(\"\\nFirst 5 predictions on TESTING data (unseen data):\")\n",
        "print(predictions_test_exercise[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bmUZv0v9f2c",
        "outputId": "5f94ec44-49df-4535-9092-db3d5a7681ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions on TESTING data (unseen data) created successfully!\n",
            "\n",
            "First 5 predictions on TESTING data (unseen data):\n",
            "[277500. 113000. 154900. 233500. 173000.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_df_test_exercise = pd.DataFrame({'Actual_UnseenData': y_test[:10].values, # Use .values to get NumPy array\n",
        "                                            'Predections (on_testing_data)': predictions_test_exercise[:10]})\n",
        "print(comparison_df_test_exercise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YccQXxlz99n3",
        "outputId": "b4bedb73-5a41-45f5-fbd3-e211c0557dbf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Actual_UnseenData  Predections (on_testing_data)\n",
            "0             305000                       277500.0\n",
            "1              80000                       113000.0\n",
            "2             100000                       154900.0\n",
            "3             235000                       233500.0\n",
            "4             171750                       173000.0\n",
            "5              76000                       103000.0\n",
            "6             188900                       200000.0\n",
            "7             209200                       153500.0\n",
            "8             255000                       197600.0\n",
            "9             174000                       167800.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: In a Markdown cell after Code Cell 27, explain your observations from Exercise 5. How do the predictions on the testing data compare to the actual values in the testing data? Is the performance on the testing data as good as the performance we saw on the training data in earlier exercises? What does this exercise demonstrate about the importance of train-test split for evaluating model generalization?\n",
        "\n",
        "    * Predictions on testing data compared to actual values looks very moderate but some of the predictions are little bit far away from reality.\n",
        "    * The performance on the testing data is as good as the preformance we saw on the training data in earlier exercises.\n",
        "    * This exercise demonstrates that train-test split gives more generalize result than not splitted datasets and gives successful result on test data."
      ],
      "metadata": {
        "id": "o6ve5uQhKV8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Critical Reflection and Future Directions (Answer in Markdown Text)\n",
        "\n",
        "1. Limitations of Visual Evaluation and Need for Metrics: In Module 3, we primarily relied on subjective visual inspection of the first 5 predictions to compare model performance. Explain why this is not a robust or reliable way to evaluate Machine Learning models in practice. What are the limitations of subjective evaluation? What do you think would be a better way to quantitatively measure and compare model performance? (Hint: Think about what we will learn in Module 4).\n",
        "\n",
        "    * Evaluating only the first 5 predictions is not representative of the model's performance because it misses patterns from big observations and its side/adverse effects are apparent in a larger dataset. Therefore, this is not a robust or reliable way to evaluate Machine Learning models in practice.\n",
        "    * Subjective evaluation varies according to people' insights and approaches so what one person considers a good prediction, another might not.\n",
        "    * Recall, precision, f1 score, accuracy, mean square error and many more would be a better way to quantitatively measure and compare model performance.\n",
        "2. Practical Implications of Overfitting and Generalization - Real-World Example: Imagine you are building a Machine Learning model to predict customer churn for a subscription-based online service. Explain, in a practical scenario, how overfitting to the training data could lead to negative consequences when you deploy this churn prediction model in the real world. Why is generalization to new customer data so crucial for the success of such a model?\n",
        "\n",
        "    * In the overfitting, the model makes incorrect churn predictions because of too focusing on training data. It causes to spend time and money on retention efforts for not risked customers or it causes to label churning customers as reliable ones, which leading to lost revenue and customer attrition.\n",
        "    * Generalized model can adapt to different customer bahaviors and patterns by providing reliable predictions that help companies to grow sustainably."
      ],
      "metadata": {
        "id": "5pd2HYSSBOQA"
      }
    }
  ]
}