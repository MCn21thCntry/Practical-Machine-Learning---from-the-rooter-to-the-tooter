{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5407,
          "databundleVersionId": 868283,
          "sourceType": "competition"
        },
        {
          "sourceId": 4852390,
          "sourceType": "datasetVersion",
          "datasetId": 33080
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Module 7 Comprehensive Advanced Ensemble Methods",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MCn21thCntry/Practical-Machine-Learning---from-the-rooter-to-the-tooter/blob/main/Module_7_Comprehensive_Advanced_Ensemble_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "sbBjpJnIucyj"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "house_prices_advanced_regression_techniques_path = kagglehub.competition_download('house-prices-advanced-regression-techniques')\n",
        "nehalbirla_vehicle_dataset_from_cardekho_path = kagglehub.dataset_download('nehalbirla/vehicle-dataset-from-cardekho')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "8navTeOeucym"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:43:45.496859Z",
          "iopub.execute_input": "2025-04-10T21:43:45.497157Z",
          "iopub.status.idle": "2025-04-10T21:43:46.72181Z",
          "shell.execute_reply.started": "2025-04-10T21:43:45.497128Z",
          "shell.execute_reply": "2025-04-10T21:43:46.720676Z"
        },
        "id": "6OByPxGaucyn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Dive into Random Forests and Bagging Versatility - Regression and Classification"
      ],
      "metadata": {
        "id": "fb8FH9NMucyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A: Hyperparameter Tuning Challenge (Regression or Classification)\n",
        "\n",
        "* ### Select either the House Price Regression task OR the Digits Classification task from this module.\n",
        "* I chose both of them as follow."
      ],
      "metadata": {
        "id": "1s5btkmkucyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "digits_df = load_digits()\n",
        "digits_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:43:46.722866Z",
          "iopub.execute_input": "2025-04-10T21:43:46.723375Z",
          "iopub.status.idle": "2025-04-10T21:43:48.219551Z",
          "shell.execute_reply.started": "2025-04-10T21:43:46.72334Z",
          "shell.execute_reply": "2025-04-10T21:43:48.2183Z"
        },
        "id": "8y1gDlSUucyp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_digits, y_digits = digits_df.data, digits_df.target"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:43:48.220574Z",
          "iopub.execute_input": "2025-04-10T21:43:48.221108Z",
          "iopub.status.idle": "2025-04-10T21:43:48.225724Z",
          "shell.execute_reply.started": "2025-04-10T21:43:48.221061Z",
          "shell.execute_reply": "2025-04-10T21:43:48.224489Z"
        },
        "id": "KEyRB-BRucyp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "X_train_digits, X_val_digits, y_train_digits, y_val_digits = train_test_split(X_digits, y_digits, test_size=0.2, random_state=21)\n",
        "\n",
        "y = house_df[\"SalePrice\"]\n",
        "numeric_features = house_df.select_dtypes(include=np.number)\n",
        "X = numeric_features.drop(\"SalePrice\", axis=1)\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X = imputer.fit_transform(X)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=21)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:43:48.228032Z",
          "iopub.execute_input": "2025-04-10T21:43:48.22846Z",
          "iopub.status.idle": "2025-04-10T21:43:48.630291Z",
          "shell.execute_reply.started": "2025-04-10T21:43:48.228421Z",
          "shell.execute_reply": "2025-04-10T21:43:48.629184Z"
        },
        "id": "0sQWvHl5ucyq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **Systematic Hyperparameter Tuning:** Instead of manual exploration, use a systematic approach to hyperparameter tuning. Choose either **Grid Search** (GridSearchCV) or **Randomized Search** (RandomizedSearchCV) from scikit-learn to tune hyperparameters for RandomForestRegressor (or RandomForestClassifier). Tune at least **three** hyperparameters (e.g., n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, criterion, bootstrap)."
      ],
      "metadata": {
        "id": "wuCd2QHOucyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RandomForest"
      ],
      "metadata": {
        "id": "I48-fGquucyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "rf_cls = RandomForestClassifier(random_state=21)\n",
        "rf_reg = RandomForestRegressor(random_state=21)\n",
        "\n",
        "rf_cls.fit(X_train_digits, y_train_digits)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "rf_cls_accuracy = accuracy_score(y_val_digits, rf_cls.predict(X_val_digits))\n",
        "rf_reg_mae = mean_absolute_error(y_val, rf_reg.predict(X_val))\n",
        "rf_reg_mse = mean_squared_error(y_val, rf_reg.predict(X_val))\n",
        "\n",
        "print(\"rf_cls_accuracy: \", rf_cls_accuracy)\n",
        "print(\"rf_reg_mae: \", rf_reg_mae)\n",
        "print(\"rf_reg_rmse: \", np.sqrt(rf_reg_mse))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:43:48.632032Z",
          "iopub.execute_input": "2025-04-10T21:43:48.632422Z",
          "iopub.status.idle": "2025-04-10T21:43:50.671985Z",
          "shell.execute_reply.started": "2025-04-10T21:43:48.632387Z",
          "shell.execute_reply": "2025-04-10T21:43:50.671018Z"
        },
        "id": "QO6lWRWiucyr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Clf parameters: \", rf_cls.get_params())\n",
        "print(\"Clf parameters: \", rf_cls.n_estimators, rf_cls.max_depth, rf_cls.max_features)\n",
        "print(\"Reg parameters: \", rf_reg.get_params())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:43:50.673006Z",
          "iopub.execute_input": "2025-04-10T21:43:50.673403Z",
          "iopub.status.idle": "2025-04-10T21:43:50.68093Z",
          "shell.execute_reply.started": "2025-04-10T21:43:50.673368Z",
          "shell.execute_reply": "2025-04-10T21:43:50.679821Z"
        },
        "id": "4FfG-nr2ucyr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Evaluate Best Model:** After tuning, evaluate the performance of your \"best\" Random Forest model (found by Grid Search or Randomized Search) on the validation set (or a separate test set if you create one). Report the best hyperparameters found and the corresponding performance (MAE for regression, Accuracy for classification).\n",
        "    * **Compare to Baseline:** Compare the performance of your tuned Random Forest model to the baseline Random Forest model (with default hyperparameters) from this module and to the best performing Bagging model you identified in your performance comparison analysis. Did hyperparameter tuning significantly improve performance beyond Bagging or the baseline Random Forest?"
      ],
      "metadata": {
        "id": "luZ1QoMuucyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "clf_param_grid = {\"n_estimators\": [50, 100, 250],\n",
        "                 \"max_depth\": [None, 10, 20, 30],\n",
        "                 \"max_features\": [\"sqrt\", \"log2\"]}\n",
        "reg_param_dist = {\"n_estimators\": [50, 100, 150, 200],\n",
        "                 \"max_depth\": [None, 10, 20, 30, 40],\n",
        "                 \"min_samples_split\": [2, 5, 10],\n",
        "                 \"min_samples_leaf\": [1, 2, 4],\n",
        "                 \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=21),\n",
        "                          clf_param_grid,\n",
        "                          cv=5,\n",
        "                          scoring=\"accuracy\",\n",
        "                          n_jobs=-1)\n",
        "grid_search.fit(X_train_digits, y_train_digits)\n",
        "print(\"RF Classification best params: \",    grid_search.best_params_)\n",
        "tuned_clf_accuracy = accuracy_score(y_val_digits, grid_search.predict(X_val_digits))\n",
        "print(\"Tuned Classification accuracy: \",    tuned_clf_accuracy)\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=21),\n",
        "                              param_distributions=reg_param_dist,\n",
        "                              n_iter=20, # # number of parameter settings that are sampled\n",
        "                              cv=5,\n",
        "                              scoring=\"neg_mean_absolute_error\",\n",
        "                              random_state=21,\n",
        "                              n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"RF Regression best params: \",    random_search.best_params_)\n",
        "tuned_reg_mae = mean_absolute_error(y_val, random_search.predict(X_val))\n",
        "print(\"Tuned Regression mae: \",    tuned_reg_mae)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:43:50.682012Z",
          "iopub.execute_input": "2025-04-10T21:43:50.682568Z",
          "iopub.status.idle": "2025-04-10T21:44:43.41389Z",
          "shell.execute_reply.started": "2025-04-10T21:43:50.682524Z",
          "shell.execute_reply": "2025-04-10T21:44:43.412842Z"
        },
        "id": "bzMjfpg5ucyr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It looks like hyperparameter optimization/tuning has unintended blowback! However, dont forget that we couldn't perpend on preprocessing stage which the most important part of ml modelling."
      ],
      "metadata": {
        "id": "2iF5HCvYucyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging"
      ],
      "metadata": {
        "id": "dj6rbnvFucys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "\n",
        "clf = LazyClassifier(predictions=True)\n",
        "clf_models, clf_predictions = clf.fit(X_train_digits, X_val_digits, y_train_digits, y_val_digits)\n",
        "print(f\"Classification Models Performance:\\n {clf_models}\")\n",
        "\n",
        "reg = LazyRegressor(predictions=True)\n",
        "reg_models, reg_predicitions = reg.fit(X_train, X_val, y_train, y_val)\n",
        "print(f\"Regression Models Performance:\\n {reg_models}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:44:43.415226Z",
          "iopub.execute_input": "2025-04-10T21:44:43.415546Z",
          "iopub.status.idle": "2025-04-10T21:45:17.759063Z",
          "shell.execute_reply.started": "2025-04-10T21:44:43.415518Z",
          "shell.execute_reply": "2025-04-10T21:45:17.757935Z"
        },
        "id": "X3g5AryHucys"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.liner_model import LinearRegression, LogisticRegression\n",
        "#from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "\"\"\"\n",
        "The following models seem the best models according to LazyReg&Clas.\n",
        "So Let's evaluate which one turns better result to use generating Bagging Model\n",
        "\"\"\"\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from lightgbm import LGBMRegressor, LGBMClassifier\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from sklearn.tree import ExtraTreeRegressor, ExtraTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "\n",
        "KN_clf_model = KNeighborsClassifier()\n",
        "KN_clf_model.fit(X_train_digits, y_train_digits)\n",
        "KN_reg_model = KNeighborsRegressor()\n",
        "KN_reg_model.fit(X_train, y_train)\n",
        "\n",
        "Lgbm_clf_model = LGBMClassifier(verbosity=0)\n",
        "Lgbm_clf_model.fit(X_train_digits, y_train_digits)\n",
        "Lgbm_reg_model = LGBMRegressor(verbosity=0)\n",
        "Lgbm_reg_model.fit(X_train, y_train)\n",
        "\n",
        "Xgboost_clf_model = XGBClassifier()\n",
        "Xgboost_clf_model.fit(X_train_digits, y_train_digits)\n",
        "Xgboost_reg_model = XGBRegressor()\n",
        "Xgboost_reg_model.fit(X_train, y_train)\n",
        "\n",
        "Extra_clf_model = ExtraTreeClassifier()\n",
        "Extra_clf_model.fit(X_train_digits, y_train_digits)\n",
        "Extra_reg_model = ExtraTreeRegressor()\n",
        "Extra_reg_model.fit(X_train, y_train)\n",
        "\n",
        "RForest_clf_model = RandomForestClassifier()\n",
        "RForest_clf_model.fit(X_train_digits, y_train_digits)\n",
        "RForest_reg_model = RandomForestRegressor()\n",
        "RForest_reg_model.fit(X_train, y_train)\n",
        "\n",
        "SVC_model = SVC()\n",
        "SVC_model.fit(X_train_digits, y_train_digits)\n",
        "SVR_model = SVR()\n",
        "SVR_model.fit(X_train, y_train)\n",
        "\n",
        "classifiers = [KN_clf_model, Lgbm_clf_model, Xgboost_clf_model, Extra_clf_model, RForest_clf_model, SVC_model]\n",
        "regressors = [KN_reg_model, Lgbm_reg_model, Xgboost_reg_model, Extra_reg_model, RForest_reg_model, SVR_model]\n",
        "\n",
        "classifiers_accuracies = {}\n",
        "for clf in classifiers:\n",
        "    accuracy = accuracy_score(y_val_digits, clf.predict(X_val_digits))\n",
        "    classifiers_accuracies[clf.__class__.__name__] = accuracy\n",
        "    print(f\"{clf.__class__.__name__} Accuracy: {accuracy}\")\n",
        "\n",
        "regressors_errors = {}\n",
        "for reg in regressors:\n",
        "    predictions = reg.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, predictions)\n",
        "    rmse = mean_squared_error(y_val, predictions, squared=False)  # calculate the RMSE\n",
        "    print(f\"{reg.__class__.__name__} MAE: {mae}\")\n",
        "    print(f\"{reg.__class__.__name__} RMSE: {rmse}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:45:17.760245Z",
          "iopub.execute_input": "2025-04-10T21:45:17.761188Z",
          "iopub.status.idle": "2025-04-10T21:45:21.503311Z",
          "shell.execute_reply.started": "2025-04-10T21:45:17.761141Z",
          "shell.execute_reply": "2025-04-10T21:45:21.502398Z"
        },
        "id": "7NHcO3Kzucys"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## KNeighborsClassifier is the best for Classification for digits dataset\n",
        "## RandomForestRegressor is the best for Regression for Home dataset\n",
        "## Then Let's use:\n",
        "from sklearn.ensemble import BaggingRegressor, BaggingClassifier\n",
        "\n",
        "bagg_cls = BaggingClassifier(base_estimator=KNeighborsClassifier(), n_estimators=10, random_state=21)\n",
        "bagg_cls.fit(X_train_digits, y_train_digits)\n",
        "bagg_cls_predictions = bagg_cls.predict(X_val_digits)\n",
        "bagg_cls_accuracy = accuracy_score(y_val_digits, bagg_cls_predictions)\n",
        "print(f\"Accuracy: {bagg_cls_accuracy}\")\n",
        "\n",
        "bagg_reg = BaggingRegressor(base_estimator=RandomForestRegressor(), n_estimators=10, random_state=21)\n",
        "bagg_reg.fit(X_train, y_train)\n",
        "bagg_reg_predictions = bagg_reg.predict(X_val)\n",
        "bagg_reg_rmse = mean_squared_error(y_val, bagg_reg_predictions, squared=False)\n",
        "print(f\"RMSE: {bagg_reg_rmse}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:45:21.504505Z",
          "iopub.execute_input": "2025-04-10T21:45:21.504908Z",
          "iopub.status.idle": "2025-04-10T21:45:30.679273Z",
          "shell.execute_reply.started": "2025-04-10T21:45:21.50487Z",
          "shell.execute_reply": "2025-04-10T21:45:30.678126Z"
        },
        "id": "y0Yn-ZMLucys"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### KNeighborsClassifier and RandomForestRegressor have given the best results so far, even better than BaggingClassifier and BaggingRegressor."
      ],
      "metadata": {
        "id": "2QZlL4wducys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNeighborsclf_param_grid = {\"n_neighbors\": [5, 10, 20, 30],\n",
        "                 \"weights\": [\"uniform\", \"distance\"],\n",
        "                 \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
        "                 \"leaf_size\": [10, 20, 30, 40],\n",
        "                 \"p\": [1, 2]} # Manhattan Distance or Euclidean Distance\n",
        "RForestreg_param_dist = {\"n_estimators\": [50, 100, 150, 200],\n",
        "                 \"max_depth\": [None, 10, 20, 30, 40],\n",
        "                 \"min_samples_split\": [2, 5, 10],\n",
        "                 \"min_samples_leaf\": [1, 2, 4],\n",
        "                 \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(),\n",
        "                          KNeighborsclf_param_grid,\n",
        "                          cv=5,\n",
        "                          scoring=\"accuracy\",\n",
        "                          n_jobs=-1)\n",
        "grid_search.fit(X_train_digits, y_train_digits)\n",
        "print(\"RF Classification best params: \",    grid_search.best_params_)\n",
        "tuned_clf_accuracy = accuracy_score(y_val_digits, grid_search.predict(X_val_digits))\n",
        "print(\"Tuned Classification accuracy: \",    tuned_clf_accuracy)\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=21),\n",
        "                              param_distributions = RForestreg_param_dist,\n",
        "                              n_iter=20, # # number of parameter settings that are sampled\n",
        "                              cv=5,\n",
        "                              scoring=\"neg_mean_absolute_error\",\n",
        "                              random_state=21,\n",
        "                              n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"RF Regression best params: \",    random_search.best_params_)\n",
        "tuned_reg_mae = mean_absolute_error(y_val, random_search.predict(X_val))\n",
        "print(\"Tuned Regression mae: \",    tuned_reg_mae)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:45:30.680291Z",
          "iopub.execute_input": "2025-04-10T21:45:30.680672Z",
          "iopub.status.idle": "2025-04-10T21:46:20.116608Z",
          "shell.execute_reply.started": "2025-04-10T21:45:30.680636Z",
          "shell.execute_reply": "2025-04-10T21:46:20.11549Z"
        },
        "id": "LQwKuGv4ucys"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ## As a result, the best result of KNeighborsClassifier remains unchanged with GridSearchCV so there is no improvement in the best result. RandomForestRegressor had given its best results as 17699.195 and has given worse with RandomizedSearchCV."
      ],
      "metadata": {
        "id": "r4UENQ6yucyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B: Explore a New Dataset with Random Forests (Regression or Classification)\n",
        "* ### Choose a new dataset: Find a publicly available dataset suitable for either regression or classification (Kaggle, UCI Machine Learning Repository, etc.). Ensure it's manageable in size and complexity.\n",
        "* ### Data Preparation: Load and prepare your chosen dataset: data cleaning, feature selection, handling missing values, train/validation split."
      ],
      "metadata": {
        "id": "H_OWzA3jucyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_vehicle = pd.read_csv(\"/kaggle/input/vehicle-dataset-from-cardekho/car data.csv\")\n",
        "df_vehicle.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:20.11771Z",
          "iopub.execute_input": "2025-04-10T21:46:20.11809Z",
          "iopub.status.idle": "2025-04-10T21:46:20.147924Z",
          "shell.execute_reply.started": "2025-04-10T21:46:20.118042Z",
          "shell.execute_reply": "2025-04-10T21:46:20.146865Z"
        },
        "id": "ep5e5YjKucyt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_vehicle.shape # (301, 9)\n",
        "df_vehicle.info() # there is no null"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:20.151139Z",
          "iopub.execute_input": "2025-04-10T21:46:20.151454Z",
          "iopub.status.idle": "2025-04-10T21:46:20.17314Z",
          "shell.execute_reply.started": "2025-04-10T21:46:20.151427Z",
          "shell.execute_reply": "2025-04-10T21:46:20.171858Z"
        },
        "id": "d-usR3vRucyt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_vehicle.describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:20.174613Z",
          "iopub.execute_input": "2025-04-10T21:46:20.174931Z",
          "iopub.status.idle": "2025-04-10T21:46:20.211556Z",
          "shell.execute_reply.started": "2025-04-10T21:46:20.174902Z",
          "shell.execute_reply": "2025-04-10T21:46:20.210494Z"
        },
        "id": "svAVCSyOucyt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "9oXDiE9Qucyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### Univariate Analysis"
      ],
      "metadata": {
        "id": "ighuFNGPucyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## There is no missing values.\n",
        "# Let's find Age of cars:\n",
        "df_vehicle[\"Age\"] = 2025 - df_vehicle[\"Year\"]\n",
        "df_vehicle.drop(\"Year\", axis=1, inplace=True)\n",
        "df_vehicle.select_dtypes(include=\"object\").columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:20.212864Z",
          "iopub.execute_input": "2025-04-10T21:46:20.213277Z",
          "iopub.status.idle": "2025-04-10T21:46:20.223277Z",
          "shell.execute_reply.started": "2025-04-10T21:46:20.21324Z",
          "shell.execute_reply": "2025-04-10T21:46:20.222168Z"
        },
        "id": "kwJFfPw5ucyt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sbn\n",
        "cat_cols = df_vehicle.select_dtypes(include=\"object\").columns.tolist()\n",
        "cat_cols.append(\"Owner\")\n",
        "cat_cols = [col for col in cat_cols if col != \"Car_Name\"]\n",
        "i=0\n",
        "while i < 4:\n",
        "    fig = plt.figure(figsize=[10,4])\n",
        "    #ax1 = fig.add_subplot(121)\n",
        "    #ax2 = fig.add_subplot(122)\n",
        "\n",
        "    #ax1.title.set_text(cat_cols[i])\n",
        "    plt.subplot(1,2,1)\n",
        "    sbn.countplot(x=cat_cols[i], data=df_vehicle)\n",
        "    i += 1\n",
        "\n",
        "    #ax2.title.set_text(cat_cols[i])\n",
        "    plt.subplot(1,2,2)\n",
        "    sbn.countplot(x=cat_cols[i], data=df_vehicle)\n",
        "    i += 1\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:20.224472Z",
          "iopub.execute_input": "2025-04-10T21:46:20.224886Z",
          "iopub.status.idle": "2025-04-10T21:46:21.23288Z",
          "shell.execute_reply.started": "2025-04-10T21:46:20.224848Z",
          "shell.execute_reply": "2025-04-10T21:46:21.231959Z"
        },
        "id": "wcb4z8s1ucyu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = df_vehicle.select_dtypes(exclude = \"object\").columns.tolist()\n",
        "num_cols = [col for col in num_cols if col != \"Owner\"]\n",
        "\n",
        "i=0\n",
        "while i < 4:\n",
        "    fig = plt.figure(figsize=[13,3])\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax2 = fig.add_subplot(122)\n",
        "\n",
        "    ax1.title.set_text(num_cols[i])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sbn.boxplot(x=num_cols[i], data = df_vehicle)\n",
        "    i += 1\n",
        "\n",
        "    ax2.title.set_text(num_cols[i])\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sbn.boxplot(x=num_cols[i], data = df_vehicle)\n",
        "    i += 1\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:21.233856Z",
          "iopub.execute_input": "2025-04-10T21:46:21.234673Z",
          "iopub.status.idle": "2025-04-10T21:46:21.702922Z",
          "shell.execute_reply.started": "2025-04-10T21:46:21.234642Z",
          "shell.execute_reply": "2025-04-10T21:46:21.701497Z"
        },
        "id": "y9f25SlKucyu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_vehicle.describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:21.703903Z",
          "iopub.execute_input": "2025-04-10T21:46:21.70421Z",
          "iopub.status.idle": "2025-04-10T21:46:21.726714Z",
          "shell.execute_reply.started": "2025-04-10T21:46:21.704182Z",
          "shell.execute_reply": "2025-04-10T21:46:21.725827Z"
        },
        "id": "_BtJgyJpucyu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for col in num_cols:\n",
        "    print(f\"{col} 0.99:\\n {df_vehicle[df_vehicle[col] > df_vehicle[col].quantile(0.99)]} \\n\")\n",
        "    print(f\"{col} 0.01:\\n {df_vehicle[df_vehicle[col] < df_vehicle[col].quantile(0.01)]} \\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:21.72778Z",
          "iopub.execute_input": "2025-04-10T21:46:21.728052Z",
          "iopub.status.idle": "2025-04-10T21:46:21.768156Z",
          "shell.execute_reply.started": "2025-04-10T21:46:21.72803Z",
          "shell.execute_reply": "2025-04-10T21:46:21.767178Z"
        },
        "id": "r47k5R03ucyv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### Bivariate/Multi-Variate Analysis"
      ],
      "metadata": {
        "id": "cXHC3xvLucyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap = sbn.heatmap(df_vehicle[num_cols].corr(), annot=True, cmap=\"RdBu\")\n",
        "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90) # Rotate x-axis to 90 for vertical\n",
        "heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0) # Rotate y-axis to 0 for horizontal\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:21.76901Z",
          "iopub.execute_input": "2025-04-10T21:46:21.769304Z",
          "iopub.status.idle": "2025-04-10T21:46:22.023511Z",
          "shell.execute_reply.started": "2025-04-10T21:46:21.769281Z",
          "shell.execute_reply": "2025-04-10T21:46:22.022481Z"
        },
        "id": "bA7BnjRVucyv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:22.024615Z",
          "iopub.execute_input": "2025-04-10T21:46:22.025006Z",
          "iopub.status.idle": "2025-04-10T21:46:22.030988Z",
          "shell.execute_reply.started": "2025-04-10T21:46:22.024969Z",
          "shell.execute_reply": "2025-04-10T21:46:22.030179Z"
        },
        "id": "gktaTt0Oucyv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's look at cat_cols' relationships with Selling_Price\n",
        "i = 0\n",
        "while i < len(cat_cols)-1:\n",
        "    for j in range(i+1, len(cat_cols)):\n",
        "        print(df_vehicle.pivot_table(values=\"Selling_Price\", index=cat_cols[i], columns = cat_cols[j]))\n",
        "        print()\n",
        "    i += 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:22.032124Z",
          "iopub.execute_input": "2025-04-10T21:46:22.032515Z",
          "iopub.status.idle": "2025-04-10T21:46:22.107399Z",
          "shell.execute_reply.started": "2025-04-10T21:46:22.032487Z",
          "shell.execute_reply": "2025-04-10T21:46:22.106352Z"
        },
        "id": "cE21HoDuucyv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "naIkyhurucyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Car_Name as it is not useful\n",
        "df_vehicle.drop(\"Car_Name\", axis=1, inplace=True)\n",
        "# One hot encoding to convert cats to nums\n",
        "df_vehicle = pd.get_dummies(data = df_vehicle, drop_first=True)\n",
        "bool_cols = df_vehicle.select_dtypes(include=['bool']).columns\n",
        "df_vehicle[bool_cols] = df_vehicle[bool_cols].astype(int)\n",
        "df_vehicle.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:22.108303Z",
          "iopub.execute_input": "2025-04-10T21:46:22.1087Z",
          "iopub.status.idle": "2025-04-10T21:46:22.127864Z",
          "shell.execute_reply.started": "2025-04-10T21:46:22.108671Z",
          "shell.execute_reply": "2025-04-10T21:46:22.126705Z"
        },
        "id": "8e9NyfDbucyw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle the outliers"
      ],
      "metadata": {
        "id": "aDebLxP0ucy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_thresholds(df, col_name, q1=0.25, q3=0.75):\n",
        "    quartile1 = df[col_name].quantile(q1)\n",
        "    quartile3 = df[col_name].quantile(q3)\n",
        "    interquantile_range = quartile3 - quartile1\n",
        "    up_limit = quartile3 + 1.5*interquantile_range\n",
        "    low_limit = quartile1 - 1.5*interquantile_range\n",
        "    return low_limit, up_limit\n",
        "\n",
        "def check_outlier(df, col_name):\n",
        "    low_limit, up_limit = outlier_thresholds(df, col_name)\n",
        "    if df[(df[col_name]<low_limit) | df[col_name]>up_limit].any(axis=None):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "for col in num_cols:\n",
        "    print(f\"{col}: {check_outlier(df_vehicle, col)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:22.129036Z",
          "iopub.execute_input": "2025-04-10T21:46:22.12938Z",
          "iopub.status.idle": "2025-04-10T21:46:22.158189Z",
          "shell.execute_reply.started": "2025-04-10T21:46:22.129353Z",
          "shell.execute_reply": "2025-04-10T21:46:22.157125Z"
        },
        "id": "V6M2_inUucy0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### There is no outliers so no need to touch data and no need to apply Local Outlier Factor(LOF)."
      ],
      "metadata": {
        "id": "I0evNuV3ucy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split"
      ],
      "metadata": {
        "id": "YhCIEhI5ucy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_vehicle[\"Selling_Price\"]\n",
        "X = df_vehicle.drop(\"Selling_Price\", axis=1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "print(\"x train: \",X_train.shape)\n",
        "print(\"x test: \",X_test.shape)\n",
        "print(\"y train: \",y_train.shape)\n",
        "print(\"y test: \",y_test.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:22.159313Z",
          "iopub.execute_input": "2025-04-10T21:46:22.159689Z",
          "iopub.status.idle": "2025-04-10T21:46:22.169975Z",
          "shell.execute_reply.started": "2025-04-10T21:46:22.159653Z",
          "shell.execute_reply": "2025-04-10T21:46:22.168969Z"
        },
        "id": "6FyAjoMaucy1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **Random Forest Modeling and Bagging Comparison:** Choose either RandomForestRegressor or RandomForestClassifier and train a baseline model (default hyperparameters) on your new dataset.Train a Bagging ensemble using Decision Tree as the base estimator on your new dataset.Evaluate and compare the performance of the Random Forest and Bagging Decision Tree models using appropriate metrics.Perform basic hyperparameter tuning (at least for n_estimators and max_depth) for the Random Forest on your new dataset to try to optimize performance."
      ],
      "metadata": {
        "id": "hJqAutJqucy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "Rforest_model = RandomForestRegressor(random_state=21)\n",
        "Rforest_model.fit(X_train, y_train)\n",
        "RF_predictions = Rforest_model.predict(X_test)\n",
        "Rforest_Rmse = mean_squared_error(y_test, RF_predictions, squared=False)\n",
        "print(f\"Random Forest RMSE: {Rforest_Rmse}\")\n",
        "\n",
        "DTree_model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), random_state=21)\n",
        "DTree_model.fit(X_train, y_train)\n",
        "DT_predictions = DTree_model.predict(X_test)\n",
        "DTree_Rmse = mean_squared_error(y_test, DT_predictions, squared=False)\n",
        "print(f\"Decision Tree RMSE: {DTree_Rmse}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:22.170934Z",
          "iopub.execute_input": "2025-04-10T21:46:22.17129Z",
          "iopub.status.idle": "2025-04-10T21:46:22.393828Z",
          "shell.execute_reply.started": "2025-04-10T21:46:22.171247Z",
          "shell.execute_reply": "2025-04-10T21:46:22.392843Z"
        },
        "id": "Jl6uCf_Bucy1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Feature Importance Analysis:** Perform feature importance analysis for your Random Forest model on the new dataset, visualize importances, and interpret results in the context of your dataset."
      ],
      "metadata": {
        "id": "N6igBuwfucy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "feature_importances = pd.DataFrame({\"Value\":Rforest_model.feature_importances_, \"Feature\": X_train.columns})\n",
        "plt.figure(figsize=(10,5))\n",
        "sbn.set(font_scale=1)\n",
        "sbn.barplot(x=\"Value\", y=\"Feature\", data=feature_importances.sort_values(by=\"Value\", ascending=False))\n",
        "plt.title(\"Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:46:22.394791Z",
          "iopub.execute_input": "2025-04-10T21:46:22.395153Z",
          "iopub.status.idle": "2025-04-10T21:46:22.72901Z",
          "shell.execute_reply.started": "2025-04-10T21:46:22.395124Z",
          "shell.execute_reply": "2025-04-10T21:46:22.727884Z"
        },
        "id": "KfXNwjB_ucy1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SKxdDNQQucy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### Present_Price dominates Selling_Price of the cars. Age and Kms_Driven are following features to impact the price of the cars."
      ],
      "metadata": {
        "id": "PhOvuiumucy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C: (Optional) Explore Bagging with Other Base Estimators (Expanded)\n",
        "\n",
        "\n",
        "* Select either the House Price Regression task OR the Digits Classification task from this module.\n",
        "* **Experiment with Bagging and Different Base Estimators (Expanded):**\n",
        "    \n",
        "    * Choose at least three different base estimators from scikit-learn other than Decision Trees, Linear Regression, Logistic Regression, KNN.\n",
        "    * For each chosen base estimator, create a Bagging ensemble using BaggingRegressor (or BaggingClassifier). Train and evaluate these Bagging ensembles on your chosen dataset (House Prices or Digits).\n",
        "    * Create at least two different Bagging ensembles with mixed base estimators. Experiment with different combinations of base estimators in your mixed ensembles (e.g., combine Decision Trees with two different new base estimators, or create a mix of three or four different types). Train and evaluate these mixed ensembles.\n",
        "    * Compare the performance of all your Bagging ensembles (with different single base estimators and mixed estimators) to the baseline Bagging Decision Tree (or Bagging Logistic Regression) from Module 7 and to Random Forest.\n",
        "    * Analyze and explain your findings: Which new base estimators worked well within Bagging? Did any of your mixed ensembles show improved performance? What are your overall conclusions about the versatility of Bagging and the impact of base estimator choices, especially when exploring less common base estimator types within Bagging?\n"
      ],
      "metadata": {
        "id": "iVZJl00pucy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.tree import ExtraTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "house_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
        "y = house_df[\"SalePrice\"]\n",
        "numeric_features = house_df.select_dtypes(include=np.number)\n",
        "X = numeric_features.drop(\"SalePrice\", axis=1)\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X = imputer.fit_transform(X)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "\n",
        "# Base Estimators for Bagging\n",
        "RFbagg_reg = BaggingRegressor(base_estimator=RandomForestRegressor(), n_estimators=10, random_state=21)\n",
        "RFbagg_reg.fit(X_train, y_train)\n",
        "\n",
        "KNbagg_reg = BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=20), n_estimators=10, random_state=21)\n",
        "KNbagg_reg.fit(X_train, y_train)\n",
        "\n",
        "LGBMbagg_reg = BaggingRegressor(base_estimator=LGBMRegressor(verbosity=0), n_estimators=10, random_state=21)\n",
        "LGBMbagg_reg.fit(X_train, y_train)\n",
        "\n",
        "XGBbagg_reg = BaggingRegressor(base_estimator=XGBRegressor(), n_estimators=10, random_state=21)\n",
        "XGBbagg_reg.fit(X_train, y_train)\n",
        "\n",
        "ExtraTbagg_reg = BaggingRegressor(base_estimator=ExtraTreeRegressor(), n_estimators=10, random_state=21)\n",
        "ExtraTbagg_reg.fit(X_train, y_train)\n",
        "\n",
        "SVRbagg_reg = BaggingRegressor(base_estimator=SVR(), n_estimators=10, random_state=21)\n",
        "SVRbagg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Mixed Ensembles\n",
        "mixed_reg1 = BaggingRegressor(base_estimator=SVR(), n_estimators=5, random_state=21)\n",
        "mixed_reg2 = BaggingRegressor(base_estimator=GradientBoostingRegressor(), n_estimators=5, random_state=21)\n",
        "\n",
        "mixed_bagging1 = BaggingRegressor(mixed_reg1, n_estimators=5, random_state=21)\n",
        "mixed_bagging1.fit(X_train, y_train)\n",
        "\n",
        "mixed_bagging2 = BaggingRegressor(mixed_reg2, n_estimators=5, random_state=21)\n",
        "mixed_bagging2.fit(X_train, y_train)\n",
        "\n",
        "regressors = [RFbagg_reg, KNbagg_reg, LGBMbagg_reg, XGBbagg_reg, ExtraTbagg_reg, SVRbagg_reg, mixed_bagging1, mixed_bagging2]\n",
        "regressors_errors = {}\n",
        "for reg in regressors:\n",
        "    predictions = reg.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, predictions)\n",
        "    rmse = mean_squared_error(y_val, predictions, squared=False)  # calculate the RMSE\n",
        "    print(f\"{reg.__class__.__name__} MAE: {mae}\")\n",
        "    print(f\"{reg.__class__.__name__} RMSE: {rmse}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T21:50:40.716409Z",
          "iopub.execute_input": "2025-04-10T21:50:40.716829Z",
          "iopub.status.idle": "2025-04-10T21:51:03.253171Z",
          "shell.execute_reply.started": "2025-04-10T21:50:40.716799Z",
          "shell.execute_reply": "2025-04-10T21:51:03.252175Z"
        },
        "id": "dOJfrQehucy2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally XGBbagg_reg, LGBMbagg_reg and mixed_reg2 with GradientBoostingRegressor passed RandomForestRegressor that having given its best result as 17699.195 and was the best amongst regressors accordingly."
      ],
      "metadata": {
        "id": "eBLPa70mucy2"
      }
    }
  ]
}